{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASTIS_Week_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppgf690tpYWy"
      },
      "source": [
        "#Step 1- Initialization\n",
        "#Import Numpy\n",
        "import numpy as np\n",
        "np.random.seed(42) #for reproducibility\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ-wwXAGqA0k"
      },
      "source": [
        "#Step 2 - Data Generation\n",
        "#Deep learning is data hungry\n",
        "#We generate our own for simplicity. \n",
        "#For inputs a and b, we have outputs a+b, a-b and |a-b|. 10000 datum points are generated\n",
        "X_num_row, X_num_col = [2, 10000] # Row is no. of feature, col is no. of datum points\n",
        "X_raw = np.random.rand(X_num_row,X_num_col) * 100\n",
        "\n",
        "y_raw = np.concatenate(([(X_raw[0,:] + X_raw[1,:])], [(X_raw[0,:] - X_raw[1,:])], np.abs([(X_raw[0,:] - X_raw[1,:])])))\n",
        "# for input a and b, output is a+b; a-b and |a-b|\n",
        "y_num_row, y_num_col = y_raw.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25gXU1X4rfvl"
      },
      "source": [
        "#Step 3- Train-test splitting\n",
        "#Data set is split into train(70%) and testing(30%) set\n",
        "train_ratio = 0.7\n",
        "num_train_datum = int(train_ratio*X_num_col)\n",
        "X_raw_train = X_raw[:,0:num_train_datum]\n",
        "X_raw_test = X_raw[:,num_train_datum:]\n",
        "\n",
        "y_raw_train = y_raw[:,0:num_train_datum]\n",
        "y_raw_test = y_raw[:,num_train_datum:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EossIu92s95c",
        "outputId": "4545f249-a49c-48e1-a24a-b7e500573a51"
      },
      "source": [
        "#Step 4- Data Standardization\n",
        "#Data in training set is standardized so that the distribution for each feature is zero-mean and unit-variance\n",
        "class scaler:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "def get_scaler(row):\n",
        "    mean = np.mean(row)\n",
        "    std = np.std(row)\n",
        "    return scaler(mean, std)\n",
        "\n",
        "def standardize(data, scaler):\n",
        "        return (data - scaler.mean) / scaler.std\n",
        "\n",
        "def unstandardize(data, scaler):\n",
        "    return (data * scaler.std) + scaler.mean\n",
        "\n",
        "\n",
        "# Construct scalers from training set\n",
        "\n",
        "X_scalers = [get_scaler(X_raw_train[row,:]) for row in range(X_num_row)]\n",
        "X_train = np.array([standardize(X_raw_train[row,:], X_scalers[row]) for row in range(X_num_row)])\n",
        "\n",
        "y_scalers = [get_scaler(y_raw_train[row,:]) for row in range(y_num_row)]\n",
        "y_train = np.array([standardize(y_raw_train[row,:], y_scalers[row]) for row in range(y_num_row)])\n",
        "\n",
        "\n",
        "# Apply those scalers to testing set\n",
        "\n",
        "X_test = np.array([standardize(X_raw_test[row,:], X_scalers[row]) for row in range(X_num_row)])\n",
        "y_test = np.array([standardize(y_raw_test[row,:], y_scalers[row]) for row in range(y_num_row)])\n",
        "\n",
        "\n",
        "# Check if data has been standardized\n",
        "\n",
        "print([X_train[row,:].mean() for row in range(X_num_row)]) # should be close to zero\n",
        "print([X_train[row,:].std() for row in range(X_num_row)])  # should be close to one\n",
        "\n",
        "print([y_train[row,:].mean() for row in range(y_num_row)]) # should be close to zero\n",
        "print([y_train[row,:].std() for row in range(y_num_row)])  # should be close to one"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.740664837931815e-16, 5.227564413092166e-17]\n",
            "[0.9999999999999999, 1.0]\n",
            "[-4.608377171929793e-16, 1.0658141036401503e-17, 6.471014200672341e-18]\n",
            "[0.9999999999999999, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0R9t_bvuvEB",
        "outputId": "b7ef2f8d-d84a-4947-adfc-60b862fc1ce1"
      },
      "source": [
        "# Step 5- Neural Net Construction\n",
        "#We objectify a ‘layer’ using class in Python. \n",
        "#Every layer (except the input layer) has a weight matrix W, a bias vector b, and an activation function. \n",
        "#Each layer is appended to a list called neural_net. \n",
        "#That list would then be a representation of your fully connected neural network.\n",
        "\n",
        "class layer:\n",
        "    def __init__(self, layer_index, is_output, input_dim, output_dim, activation):\n",
        "        self.layer_index = layer_index # zero indicates input layer\n",
        "        self.is_output = is_output # true indicates output layer, false otherwise\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = activation\n",
        "        \n",
        "        # the multiplication constant is sorta arbitrary\n",
        "        if layer_index != 0:\n",
        "            self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n",
        "            self.b = np.random.randn(output_dim, 1) * np.sqrt(2/input_dim)\n",
        "            \n",
        "            \n",
        "# Change layers_dim to configure your own neural net!            \n",
        "layers_dim = [X_num_row, 4, 4, y_num_row] # input layer --- hidden layers --- output layers\n",
        "neural_net = []\n",
        "\n",
        "# Construct the net layer by layer\n",
        "for layer_index in range(len(layers_dim)):\n",
        "    if layer_index == 0: # if input layer\n",
        "        neural_net.append(layer(layer_index, False, 0, layers_dim[layer_index], 'irrelevant'))\n",
        "    elif layer_index+1 == len(layers_dim): # if output layer\n",
        "        neural_net.append(layer(layer_index, True, layers_dim[layer_index-1], layers_dim[layer_index], activation='linear'))\n",
        "    else: \n",
        "        neural_net.append(layer(layer_index, False, layers_dim[layer_index-1], layers_dim[layer_index], activation='relu'))\n",
        "        \n",
        "        \n",
        "# Simple check on overfitting\n",
        "\n",
        "pred_n_param = sum([(layers_dim[layer_index]+1)*layers_dim[layer_index+1] for layer_index in range(len(layers_dim)-1)])\n",
        "act_n_param = sum([neural_net[layer_index].W.size + neural_net[layer_index].b.size for layer_index in range(1,len(layers_dim))])\n",
        "print(f'Predicted number of hyperparameters: {pred_n_param}')\n",
        "print(f'Actual number of hyperparameters: {act_n_param}')\n",
        "print(f'Number of data: {X_num_col}')\n",
        "\n",
        "if act_n_param >= X_num_col:\n",
        "    raise Exception('It will overfit.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted number of hyperparameters: 47\n",
            "Actual number of hyperparameters: 47\n",
            "Number of data: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEsUUzSnxp_d"
      },
      "source": [
        "#Step 6-Forward Propagation\n",
        "#We define a function for forward propagation given a certain set of weights and biases.\n",
        "def activation(input_, act_func):\n",
        "    if act_func == 'relu':\n",
        "        return np.maximum(input_, np.zeros(input_.shape))\n",
        "    elif act_func == 'linear':\n",
        "        return input_\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')\n",
        "        \n",
        "  \n",
        "def forward_prop(input_vec, layers_dim=layers_dim, neural_net=neural_net):\n",
        "    neural_net[0].A = input_vec # Define A in input layer for for-loop convenience\n",
        "    for layer_index in range(1,len(layers_dim)): # W,b,Z,A are undefined in input layer\n",
        "        neural_net[layer_index].Z = np.add(np.dot(neural_net[layer_index].W, neural_net[layer_index-1].A), neural_net[layer_index].b)\n",
        "        neural_net[layer_index].A = activation(neural_net[layer_index].Z, neural_net[layer_index].activation)\n",
        "    return neural_net[layer_index].A  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POPvLcvf2Xnf"
      },
      "source": [
        "#Step 7 - Back-propagation\n",
        "def get_loss(y, y_hat, metric='mse'):\n",
        "    if metric == 'mse':\n",
        "        individual_loss = 0.5 * (y_hat - y) ** 2\n",
        "        return np.mean([np.linalg.norm(individual_loss[:,col], 2) for col in range(individual_loss.shape[1])])\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined.')\n",
        "\n",
        "def get_dZ_from_loss(y, y_hat, metric):\n",
        "    if metric == 'mse':\n",
        "        return y_hat - y\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined.')\n",
        "\n",
        "def get_dactivation(A, act_func):\n",
        "    if act_func == 'relu':\n",
        "        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n",
        "    elif act_func == 'linear':\n",
        "        return np.ones(A.shape)\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')\n",
        "        \n",
        "        \n",
        "def backward_prop(y, y_hat, metric='mse', layers_dim=layers_dim, neural_net=neural_net, num_train_datum=num_train_datum):\n",
        "    for layer_index in range(len(layers_dim)-1,0,-1):\n",
        "        if layer_index+1 == len(layers_dim): # if output layer\n",
        "            dZ = get_dZ_from_loss(y, y_hat, metric)\n",
        "        else: \n",
        "            dZ = np.multiply(np.dot(neural_net[layer_index+1].W.T, dZ), \n",
        "                             get_dactivation(neural_net[layer_index].A, neural_net[layer_index].activation))\n",
        "        dW = np.dot(dZ, neural_net[layer_index-1].A.T) / num_train_datum\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n",
        "        \n",
        "        neural_net[layer_index].dW = dW\n",
        "        neural_net[layer_index].db = db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX1RjNmX2_Mr",
        "outputId": "075ff408-4a76-4ce4-e295-28fba4766ebb"
      },
      "source": [
        "#Step 8 - Iterative Optimization\n",
        "#Once we know the sensitivities of weights and biases, \n",
        "#we try to minimize (hence the minus sign) the loss metric iteratively by gradient descent using the following update rule:\n",
        "#W = W - learning_rate * ∂W\n",
        "#b = b - learning_rate * ∂b\n",
        "\n",
        "learning_rate = 0.01\n",
        "max_epoch = 1000000\n",
        "\n",
        "for epoch in range(1,max_epoch+1):\n",
        "    y_hat_train = forward_prop(X_train) # update y_hat\n",
        "    backward_prop(y_train, y_hat_train) # update (dW,db)\n",
        "    \n",
        "    for layer_index in range(1,len(layers_dim)):        # update (W,b)\n",
        "        neural_net[layer_index].W = neural_net[layer_index].W - learning_rate * neural_net[layer_index].dW\n",
        "        neural_net[layer_index].b = neural_net[layer_index].b - learning_rate * neural_net[layer_index].db\n",
        "    \n",
        "    if epoch % 100000 == 0:\n",
        "        print(f'{get_loss(y_train, y_hat_train):.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3502\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k67MF5Iv7aSo",
        "outputId": "1b80cb54-a765-418e-ae8a-8ac69eb1f5d4"
      },
      "source": [
        "#Step 9 - Testing\n",
        "#The model generalizes well if the testing loss is not much higher than the training loss. \n",
        "#We also make some test cases to see how the model performs.\n",
        "\n",
        "# test loss\n",
        "\n",
        "print(get_loss(y_test, forward_prop(X_test)))\n",
        "\n",
        "def predict(X_raw_any):\n",
        "    X_any = np.array([standardize(X_raw_any[row,:], X_scalers[row]) for row in range(X_num_row)])\n",
        "    y_hat = forward_prop(X_any)\n",
        "    y_hat_any = np.array([unstandardize(y_hat[row,:], y_scalers[row]) for row in range(y_num_row)])\n",
        "    return y_hat_any\n",
        "    \n",
        "predict(np.array([[30,70],[70,30],[3,5],[888,122]]).T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3194238845483295\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 98.56378114, 102.63221611,   5.56349031, 562.15335048],\n",
              "       [-26.25088759,  14.80155747,  21.3245024 , 221.60597885],\n",
              "       [ 48.08520947,  25.18205121,  15.71597463, -64.01423955]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}